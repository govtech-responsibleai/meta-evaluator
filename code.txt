diff --git a/src/meta_evaluator/LLMClient/__init__.py b/src/meta_evaluator/LLMClient/__init__.py
new file mode 100644
index 0000000..ceadd0a
--- /dev/null
+++ b/src/meta_evaluator/LLMClient/__init__.py
@@ -0,0 +1,77 @@
+import logging
+from abc import ABC, abstractmethod
+from typing import Optional
+
+from pydantic import BaseModel
+from .models import Message, LLMClientEnum, LLMResponse
+
+
+
+
+class LLMClientConfig(ABC, BaseModel):
+    logger: logging.Logger
+    api_key: str 
+    supports_instructor: bool
+    default_model: str 
+    default_embedding_model: str
+
+
+
+
+class LLMClient(ABC):
+    def __init__(self, config: LLMClientConfig):
+        self.config = config
+        self.logger = config.logger
+
+    @abstractmethod
+    def _prompt(self, model:str, messages: list[Message]) -> LLMResponse:
+        raise NotImplementedError("Subclasses must implement this method")
+    
+    @property
+    @abstractmethod
+    def enum_value(self) -> LLMClientEnum:
+        raise NotImplementedError("Subclasses must implement this method")
+
+    def prompt(self, messages: list[Message], model: Optional[str] = None) -> LLMResponse:
+        """Send a prompt to the underlying LLM client with comprehensive logging.
+
+        This method handles model selection, logging, and delegation to the provider-specific
+        implementation. If no model is specified, falls back to the client's configured
+        default model.
+
+        Args:
+            messages (list[Message]): Complete conversation history including system, user,
+                and assistant messages. The conversation context is preserved and sent to
+                the underlying provider.
+            model (Optional[str], optional): Model to use for this request. If None,
+                uses self.config.default_model. Defaults to None.
+
+        Returns:
+            LLMResponse: Response object containing the full conversation (input + new
+                assistant response), usage statistics, and metadata. Access the latest
+                response via response.latest_response or response.content.
+
+        Note:
+            All requests are logged including:
+            - Model selection (chosen vs default)
+            - Complete input message payload
+            - Assistant response content
+            - Token usage statistics
+
+        Example:
+            >>> messages = [Message(role=RoleEnum.USER, content="Hello")]
+            >>> response = client.prompt(messages, model="gpt-4")
+            >>> print(response.content)  # Assistant's response
+            >>> print(len(response.messages))  # Original + assistant response
+        """
+        model_used = model or self.config.default_model
+        self.logger.info(f"Using model: {model_used}")
+        self.logger.info(f"Input Payload: {messages}")
+        output = self._prompt(model_used, messages)
+        self.logger.info(f"Latest response: {output.latest_response}")
+        self.logger.info(f"Output usage: {output.usage}")
+
+        return output
+
+
+
diff --git a/src/meta_evaluator/LLMClient/models.py b/src/meta_evaluator/LLMClient/models.py
new file mode 100644
index 0000000..f2bab47
--- /dev/null
+++ b/src/meta_evaluator/LLMClient/models.py
@@ -0,0 +1,57 @@
+from enum import Enum
+import random
+import time
+from typing import Any
+from pydantic import BaseModel
+
+
+class RoleEnum(str, Enum):
+    SYSTEM = "system"
+    USER = "user"
+    ASSISTANT = "assistant"
+
+class Message(BaseModel):
+    role: RoleEnum
+    content: str
+
+    def __str__(self):
+        return f"{self.role.value}: {self.content}"
+    
+
+class LLMClientEnum(Enum):
+    OPENAI = "openai"
+    AZURE_OPENAI = "azure_openai"
+    GEMINI = "gemini"
+    ANTHROPIC = "anthropic"
+
+    
+
+class LLMUsage(BaseModel):
+    prompt_tokens: int
+    completion_tokens: int
+    total_tokens: int
+
+
+class LLMResponse(BaseModel):
+    id: str  = ""
+    provider: LLMClientEnum
+    model: str
+    messages: list[Message]
+    usage: LLMUsage
+
+    @property
+    def latest_response(self) -> Message:
+        return self.messages[-1]
+    
+    @property
+    def content(self) -> str:
+        return self.latest_response.content
+    
+    
+    def model_post_init(self, __context: Any) -> None:
+        if not self.id:  
+            timestamp = int(time.time())
+            random_num = random.randint(1000, 9999)
+            self.id = f"{timestamp}_{self.provider.value}_{self.model}_{random_num}"
+        
+
diff --git a/src/meta_evaluator/__init__.py b/src/meta_evaluator/__init__.py
index 43bbe32..e7f0366 100644
--- a/src/meta_evaluator/__init__.py
+++ b/src/meta_evaluator/__init__.py
@@ -1,2 +1,3 @@
-def main() -> None:
-    print("Hello from meta-evaluator again!")
+class MetaEvaluator:
+    def __init__(self):
+        pass
\ No newline at end of file
